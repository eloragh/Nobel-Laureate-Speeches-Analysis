{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98988f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6ac14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#initialize pandas Dataframe for later use\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e0c773f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define the path where the speech files are found\n",
    "path = \"nobelspeeches\"\n",
    "nobel_speeches = os.listdir(path)\n",
    "\n",
    "#join the path with the files to find the appropriate file when looking for it\n",
    "files = sorted([os.path.join(path, file) for file in nobel_speeches if file.endswith('.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36732968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that will open the file read it, making the text available in python\n",
    "def read_file(file_name):\n",
    "  #open the file with the correct encoding and save it to a variable \"file\"\n",
    "  with open(file_name, 'r+', encoding='utf-8') as file:\n",
    "    #read the file and save it to a variable \"file_text\"\n",
    "    file_text = file.read()\n",
    "   #return the text for use outside of the function \n",
    "  return file_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ef2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the read_file function for all of the documents in the folder\n",
    "speeches = [read_file(doc) for doc in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b9fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that will process the data\n",
    "def process_speeches(speeches):\n",
    "  #create a new list to append the cleaned data to\n",
    "  word_tokenized_speeches = list()\n",
    "  #loop through each speech in the argument\n",
    "  for speech in speeches:\n",
    "    #initialize the sentence Tokenizer\n",
    "    sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    #apply the sentence tokenizer to the speech\n",
    "    sentence_tokenized_speech = sentence_tokenizer.tokenize(speech)\n",
    "    #create a new list to append the tokenized sentences too\n",
    "    word_tokenized_sentences = list()\n",
    "    #loop through each sentence in the tokenized speech\n",
    "    for sentence in sentence_tokenized_speech:\n",
    "      #strip each sentence of punctuation and split them into words\n",
    "      word_tokenized_sentence = [word.lower().strip('.').strip('?').strip('!') for word in sentence.replace(\",\",\"\").replace(\"-\",\" \").replace(\":\",\"\").split()]\n",
    "      #append the stripped sentences to the new list\n",
    "      word_tokenized_sentences.append(word_tokenized_sentence)\n",
    "    #append the cleaned sentences to the original list\n",
    "    word_tokenized_speeches.append(word_tokenized_sentences)\n",
    "  #return the original list for us outside of the function\n",
    "  return word_tokenized_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a3665a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = process_speeches(speeches)\n",
    "#print to make sure the function works (can be commented out)\n",
    "#print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "218d3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will merge all of the speeches together into one pool\n",
    "def merge_speeches(speeches):\n",
    "  #create an empty list to append the sentences too\n",
    "  all_sentences = list()\n",
    "  #loop through each speech in the processed text\n",
    "  for speech in speeches:\n",
    "    #loop through each sentence in the speech\n",
    "    for sentence in speech:\n",
    "      #append each sentence to the new list\n",
    "      all_sentences.append(sentence)\n",
    "  #return the list\n",
    "  return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa0d964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the function and print it to make sure it works (can be commented out)\n",
    "merged_speeches = merge_speeches(processed_text)\n",
    "#print(merged_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fae1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that will find each individual speakers sentences for analysis\n",
    "def get_speaker_sentences(speaker):\n",
    "  #this list comprehension concatenates both the path and the file, joining them together to access the files individually if such a file exists in the folder\n",
    "  files = sorted([os.path.join(path, file) for file in nobel_speeches if speaker.lower() in file.lower()])\n",
    "  #the speeches variable holds a list comprehension that uses the earlier read_file function to read the individual file as referenced by the speaker name\n",
    "  speeches = [read_file(file) for file in files]\n",
    "  #uses the earlier function process speech to tokenize the sentences and words while removing punctuation\n",
    "  processed_speeches = process_speeches(speeches)\n",
    "  #uses the earlier merge_speeches function to create a list of all of the sentences in the speech\n",
    "  all_sentences = merge_speeches(processed_speeches)\n",
    "  #returns the sentences for use outside of the function\n",
    "  return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c952e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the function and print it to check if it works (can be commented out)\n",
    "speaker_sentences = get_speaker_sentences(\"mother_theresa\")\n",
    "#print(speaker_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab5f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that will allow a user to look for most common words among more than one nobel lecturer\n",
    "def get_speakers_sentences(speakers):\n",
    "  #create a new list that we can append tokenized sentences to\n",
    "  all_sentences = list()\n",
    "  #loop through each speaker within speakers\n",
    "  for speaker in speakers:\n",
    "    #this list comprehension concatenates both the path and the file, joining them together to access the files individually if such a file exists in the folder\n",
    "    files = sorted([os.path.join(path, file) for file in nobel_speeches if speaker in file])\n",
    "    #the speeches variable holds a list comprehension that uses the earlier read_file function to read the individual file as referenced by the speaker name\n",
    "    speeches = [read_file(file) for file in files]\n",
    "    #uses the earlier function process speech to tokenize the sentences and words while removing punctuation\n",
    "    processed_speeches = process_speeches(speeches)\n",
    "    #uses the earlier merge_speeches function to create a list of all of the sentences in the speech\n",
    "    all_speaker_sentences = merge_speeches(processed_speeches)\n",
    "    #extend the new list with the list created from the merge_speeches function\n",
    "    all_sentences.extend(all_speaker_sentences)\n",
    "  #return the list of all_sentences for use outside of the function\n",
    "  return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29f53335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that can find all of the most frequently used words by speaker(s)\n",
    "def most_frequent_words(list_of_sentences):\n",
    "  #list comprehension finds all of the words in the sentences provided\n",
    "  all_words = [word for sentence in list_of_sentences for word in sentence]\n",
    "  #Counter attribute .most_common() is able to parse through all of the words and find the most common as well as their tally\n",
    "  return Counter(all_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add1db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the get_speaker_sentences or get_speakers_sentences function \n",
    "speakers_sentences = get_speakers_sentences([\"barack_obama\", \"mother_theresa\", \"dalai_lama\", \"martin_luther_king\"])\n",
    "#print(speakers_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "202f5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the function and print it to make sure it works (can be commented out)\n",
    "speakers_freq_words = most_frequent_words(speakers_sentences)\n",
    "#print(speakers_freq_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
