{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nobel Peace Prize Speech Analysis\n",
    "\n",
    "## This notebook walks through how I updated my initial analysis and the improvements I made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import collections as col\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the path where the speech files are found\n",
    "path = \"nobelspeeches\"\n",
    "nobel_speeches = os.listdir(path)\n",
    "\n",
    "#join the path with the files to find the appropriate file when looking for it\n",
    "files = sorted([os.path.join(path, file) for file in nobel_speeches if file.endswith('.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The stopwords provided by the NLTK corpus are a list of the most common words in each language.\n",
    "#### For English - \"the\", \"it\", \"a\", \"and\", \"when\", etc.\n",
    "#### These words have almost no contributing value to the analysis, so we use stopwords to weed those out.\n",
    "#### I added a few pieces of punctuation to the list of stopwords to weed those out as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "stops.update([\"–\",\"…\",\"*\", \" \", \"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of tokenizing the words, I used a for loop to iterate through them and pull them out of each file into a list.\n",
    "\n",
    "#### I also used regex to tackle some of the more challenging issues within the data so that the final result was as clean as possible.\n",
    "\n",
    "#### The collections module is useful for quickly forming dictionaries out of iterable datatypes and then sorting the dictionaries by their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    file1 = open(file, encoding = \"utf8\")\n",
    "    file2 = file1.read()\n",
    "    file3 = re.sub(\"\\b\\W?(\\w*)\\W?\\b\", r\"/1\", file2)\n",
    "    file4 = re.sub(r\"[\\b?\\(\\n*)](\\w*)[\\(\\n*)\\b?]\", r\" \\1\", file3)\n",
    "    file4 = re.sub(r\"\\\\n*\", \" \", file3)\n",
    "    file5 = re.sub(r\"(,|;|:|\\.|)?(\\w*)(,|;|:|\\.|)?\", r\"\\2\", file4)\n",
    "    file5.replace(\"\\n\", \" \")\n",
    "    lst = file5.lower().split(\" \")\n",
    "\n",
    "    for word in lst:\n",
    "        if word.startswith(\"\\'\"):\n",
    "            pass\n",
    "        elif word not in stops:\n",
    "            vocab_list.append(word)\n",
    "\n",
    "most_frequent_words = col.Counter(vocab_list).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of a list with a word and the occurence value, I used a dictionary and took advantage of the key:value strengths.\n",
    "\n",
    "#### This data is far cleaner and was organized in a much shorter time with far fewer lines of code.\n",
    "\n",
    "#### Uncomment the last line to see the most frequently used words of the last 29 Nobel Laureates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_frequent_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c27904bfabd9e4a7f1d5f5dcc5d2a6d0fab6e51da12729af85e6eafdbb7b7a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
